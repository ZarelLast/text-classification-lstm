# -*- coding: utf-8 -*-
"""(Dicoding) Proyek Pertama : Membuat Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lJ47rKs-cRWfim2LL_bTzDcKA38zrAUm

Dataset: Text Document Classification Dataset <br>
Link dataset: [Link](https://www.kaggle.com/datasets/sunilthite/text-document-classification-dataset) <br>
Jumlah Baris: 2225 baris <br>
Jumlah Kolom: 2 kolom <br>
Jumlah Label: 5
1. Politics = 0
2. Sport = 1
3. Technology = 2
4. Entertainment =3
5. Business = 4

This is text document classification dataset which contains 2225 text data and five categories of documents. Five categories are politics, sport, tech, entertainment and business. We can use this dataset for documents classification and document clustering.

Dataset ini merupakan teks dokumen klasifikasi yang memiliki 2225 data teks dan 5 kategori/label dokumen. 5 kategori/label tersebut adalah politik, olahraga, teknologi, entertaimen dan bisnis. Dokumen ini dapat digunakan untuk klasifikasi dokumen dan clustering dokumen.
"""

import tensorflow as tf
tf.test.gpu_device_name()

import pandas as pd
df = pd.read_csv('df_file.csv')

df.info()

df.shape

df.describe()

df.groupby('Label').size()

df

kategori = pd.get_dummies(df.Label)
df_baru = pd.concat([df, kategori], axis=1)
df_baru = df_baru.drop(columns='Label')
df_baru

df_baru['Text'][0]

teks = df_baru['Text'].values
label = df_baru[[0, 1, 2, 3, 4]].values

label[0]

teks[0]

import re
import numpy as np
teks_regex = np.array([re.sub('\W+',' ', x.lower()) for x in df_baru['Text']])

teks_regex[0]

import nltk
nltk.download('stopwords')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))

teks_bersih = np.array([x for x in teks_regex if not x in stop_words])

teks_bersih[0]

from sklearn.model_selection import train_test_split
teks_latih, teks_test, label_latih, label_test = train_test_split(teks_bersih, label, test_size=0.2, random_state=123, shuffle=True)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=26645, oov_token='<oov>')
tokenizer.fit_on_texts(teks_latih)

sekuens_latih = tokenizer.texts_to_sequences(teks_latih)
sekuens_test = tokenizer.texts_to_sequences(teks_test)

max_seqlatih = min([len(sen) for sen in sekuens_latih])
print(max_seqlatih)
max_seqtest = min([len(sen) for sen in sekuens_latih])
print(max_seqtest)

padded_latih = pad_sequences(sekuens_latih, padding='post', truncating='post', maxlen=max_seqlatih)
padded_test = pad_sequences(sekuens_test, padding='post', truncating='post', maxlen=max_seqtest)

# disini pertama saya run kode diatas kemudian print kode dibawah untuk menentukan num_words
# karena menurut artikel yang saya baca https://medium.com/analytics-vidhya/understanding-nlp-keras-tokenizer-class-arguments-with-example-551c100f0cbd
# "best value is to use for the num_words is “len(tokenizer.word_index) + 1" ".
# kemudian saya run ulang kode diatas dengan mengganti num_words
# menjadi sesuai dengan “len(tokenizer.word_index) + 1"
max_input = len(tokenizer.word_index) + 1

import tensorflow as tf
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=max_input, output_dim=128, input_length=max_seqlatih),
    tf.keras.layers.LSTM(128),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(96, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(5, activation='softmax')
])
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.summary()

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.92 and logs.get('val_accuracy')>0.92 and logs.get('val_loss')<0.3):
      print("\nAkurasi dan Val Akurasi telah mencapai >92%! serta val loss dibawah 30%")
      self.model.stop_training = True
callbacks = myCallback()

num_epochs = 300
history = model.fit(padded_latih, label_latih, epochs=num_epochs, batch_size=32,
                    validation_data=(padded_test, label_test), verbose=2, callbacks=[callbacks])

import matplotlib.pyplot as plt

plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(history.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()

plt.plot(history.history['val_loss'])
plt.title('val loss')
plt.ylabel('val_loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(history.history['val_accuracy'])
plt.title('val accuracy')
plt.ylabel('val_accuracy')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='lower right')
plt.show()